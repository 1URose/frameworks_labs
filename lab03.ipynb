{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа 3 (DecisionTree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Импорт всех необходимых модулей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import (\n",
    "    LabelEncoder,\n",
    "    StandardScaler,\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def regression_cross_validate(model_cls, X, y, n_folds: int = 5, **model_kwargs):\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    mae_values = []\n",
    "    mse_values = []\n",
    "    r2_values = []\n",
    "\n",
    "    for fold_idx, (idx_train, idx_valid) in enumerate(kf.split(X), start=1):\n",
    "        X_train, X_valid = X[idx_train], X[idx_valid]\n",
    "        y_train, y_valid = y[idx_train], y[idx_valid]\n",
    "\n",
    "        model = model_cls(**model_kwargs)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = model.predict(X_valid)\n",
    "\n",
    "        mae_values.append(mean_absolute_error(y_valid, y_pred))\n",
    "        mse_values.append(mean_squared_error(y_valid, y_pred))\n",
    "        r2_values.append(r2_score(y_valid, y_pred))\n",
    "\n",
    "    metrics_avg = {\n",
    "        \"MAE\": float(np.mean(mae_values)),\n",
    "        \"MSE\": float(np.mean(mse_values)),\n",
    "        \"R2\": float(np.mean(r2_values)),\n",
    "    }\n",
    "\n",
    "    metrics_std = {\n",
    "        \"MAE\": float(np.std(mae_values)),\n",
    "        \"MSE\": float(np.std(mse_values)),\n",
    "        \"R2\": float(np.std(r2_values)),\n",
    "    }\n",
    "\n",
    "    return metrics_avg, metrics_std\n",
    "\n",
    "\n",
    "def classification_cross_validate(model_cls, X, y, n_folds: int = 5, **model_kwargs):\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    acc_list = []\n",
    "    prec_list = []\n",
    "    rec_list = []\n",
    "    f1_list = []\n",
    "\n",
    "    for fold_idx, (idx_train, idx_valid) in enumerate(kf.split(X), start=1):\n",
    "        X_train, X_valid = X[idx_train], X[idx_valid]\n",
    "        y_train, y_valid = y[idx_train], y[idx_valid]\n",
    "\n",
    "        model = model_cls(**model_kwargs)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = model.predict(X_valid)\n",
    "\n",
    "        acc_list.append(accuracy_score(y_valid, y_pred))\n",
    "        prec_list.append(precision_score(y_valid, y_pred, average=\"weighted\", zero_division=0))\n",
    "        rec_list.append(recall_score(y_valid, y_pred, average=\"weighted\", zero_division=0))\n",
    "        f1_list.append(f1_score(y_valid, y_pred, average=\"weighted\", zero_division=0))\n",
    "\n",
    "    metrics_avg = {\n",
    "        \"Accuracy\": float(np.mean(acc_list)),\n",
    "        \"Precision\": float(np.mean(prec_list)),\n",
    "        \"Recall\": float(np.mean(rec_list)),\n",
    "        \"F1-score\": float(np.mean(f1_list)),\n",
    "    }\n",
    "\n",
    "    metrics_std = {\n",
    "        \"Accuracy\": float(np.std(acc_list)),\n",
    "        \"Precision\": float(np.std(prec_list)),\n",
    "        \"Recall\": float(np.std(rec_list)),\n",
    "        \"F1-score\": float(np.std(f1_list)),\n",
    "    }\n",
    "\n",
    "    return metrics_avg, metrics_std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 1. Регрессия\n",
    "Данные о зарплатах загружаются в DataFrame, и выводятся первые строки, чтобы убедиться, что файл прочитан корректно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер датасета Salary_Data: (375, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Education Level</th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Years of Experience</th>\n",
       "      <th>Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>5.0</td>\n",
       "      <td>90000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Master's</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>3.0</td>\n",
       "      <td>65000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>PhD</td>\n",
       "      <td>Senior Manager</td>\n",
       "      <td>15.0</td>\n",
       "      <td>150000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Sales Associate</td>\n",
       "      <td>7.0</td>\n",
       "      <td>60000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Master's</td>\n",
       "      <td>Director</td>\n",
       "      <td>20.0</td>\n",
       "      <td>200000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age  Gender Education Level          Job Title  Years of Experience  \\\n",
       "0  32.0    Male      Bachelor's  Software Engineer                  5.0   \n",
       "1  28.0  Female        Master's       Data Analyst                  3.0   \n",
       "2  45.0    Male             PhD     Senior Manager                 15.0   \n",
       "3  36.0  Female      Bachelor's    Sales Associate                  7.0   \n",
       "4  52.0    Male        Master's           Director                 20.0   \n",
       "\n",
       "     Salary  \n",
       "0   90000.0  \n",
       "1   65000.0  \n",
       "2  150000.0  \n",
       "3   60000.0  \n",
       "4  200000.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salary_df = pd.read_csv(\"data/Salary Data.csv\")\n",
    "print(\"Размер датасета Salary_Data:\", salary_df.shape)\n",
    "\n",
    "salary_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка данных для регрессии\n",
    "\n",
    "Сначала из датасета убирается столбец `Job Title`: он содержит много уникальных значений и в таком виде мало помогает линейной модели, только раздувает пространство признаков\n",
    "\n",
    "Категориальные признаки (`Gender`, `Education Level`) переводятся в числовой вид с помощью `OrdinalEncoder`, чтобы их можно было использовать в линейной регрессии и в собственной реализации модели\n",
    "\n",
    "После этого все пропуски в данных заполняются наиболее частыми значениями (`SimpleImputer` с стратегией `\"most_frequent\"`). Такой шаг нужен, потому что большинство моделей из `sklearn`, а также наши собственные реализации, не умеют работать с `NaN` и ожидают полностью числовую матрицу признаков без пропусков\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN в X_train_base: 0\n",
      "NaN в X_test_base: 0\n"
     ]
    }
   ],
   "source": [
    "reg_df = salary_df.copy()\n",
    "reg_df = reg_df.dropna(subset=[\"Salary\"])\n",
    "\n",
    "if \"Job Title\" in reg_df.columns:\n",
    "    reg_df = reg_df.drop(columns=[\"Job Title\"])\n",
    "\n",
    "X_reg = reg_df.drop(columns=[\"Salary\"])\n",
    "y_reg = reg_df[\"Salary\"]\n",
    "\n",
    "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "reg_categorical_cols = [\"Gender\", \"Education Level\"]\n",
    "reg_numeric_cols = [col for col in X_reg.columns if col not in reg_categorical_cols]\n",
    "\n",
    "reg_ordinal_encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "\n",
    "X_reg_train_base = X_reg_train.copy()\n",
    "X_reg_test_base = X_reg_test.copy()\n",
    "\n",
    "X_reg_train_base[reg_categorical_cols] = reg_ordinal_encoder.fit_transform(\n",
    "    X_reg_train_base[reg_categorical_cols]\n",
    ")\n",
    "X_reg_test_base[reg_categorical_cols] = reg_ordinal_encoder.transform(\n",
    "    X_reg_test_base[reg_categorical_cols]\n",
    ")\n",
    "\n",
    "reg_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "X_reg_train_base = pd.DataFrame(\n",
    "    reg_imputer.fit_transform(X_reg_train_base),\n",
    "    columns=X_reg_train_base.columns\n",
    ")\n",
    "X_reg_test_base = pd.DataFrame(\n",
    "    reg_imputer.transform(X_reg_test_base),\n",
    "    columns=X_reg_test_base.columns\n",
    ")\n",
    "\n",
    "print(\"NaN в X_train_base:\", X_reg_train_base.isna().sum().sum())\n",
    "print(\"NaN в X_test_base:\", X_reg_test_base.isna().sum().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Построение бейзлайна "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполняется кросс-валидация базовой модели решающего дерева для задачи регрессии и выводятся средние значения метрик на обучающих разбиениях. Затем модель обучается на тренировочной выборке, оценивается на тестовой и печатаются итоговые значения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Регрессия: бейзлайн (train CV) ===\n",
      "MSE_mean: 356229547.76\n",
      "MAE_mean: 11227.38\n",
      "R2_mean:  0.842\n",
      "\n",
      "Регрессия - бейзлайн (test)\n",
      "---------------------------\n",
      "MSE: 231101568.41\n",
      "MAE: 9428.57\n",
      "R^2: 0.904\n"
     ]
    }
   ],
   "source": [
    "reg_tree_mean, reg_tree_std = regression_cross_validate(\n",
    "    DecisionTreeRegressor,\n",
    "    X_reg_train_base.to_numpy(),\n",
    "    y_reg_train.to_numpy(),\n",
    "    n_folds=5,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"=== Регрессия: бейзлайн (train CV) ===\")\n",
    "print(f\"MSE_mean: {reg_tree_mean['MSE']:.2f}\")\n",
    "print(f\"MAE_mean: {reg_tree_mean['MAE']:.2f}\")\n",
    "print(f\"R2_mean:  {reg_tree_mean['R2']:.3f}\")\n",
    "\n",
    "dt_baseline = DecisionTreeRegressor(random_state=42)\n",
    "dt_baseline.fit(X_reg_train_base, y_reg_train)\n",
    "\n",
    "y_reg_pred_tree = dt_baseline.predict(X_reg_test_base)\n",
    "\n",
    "mse_tree = mean_squared_error(y_reg_test, y_reg_pred_tree)\n",
    "mae_tree = mean_absolute_error(y_reg_test, y_reg_pred_tree)\n",
    "r2_tree = r2_score(y_reg_test, y_reg_pred_tree)\n",
    "\n",
    "print(\"\\nРегрессия - бейзлайн (test)\")\n",
    "print(\"---------------------------\")\n",
    "print(f\"MSE: {mse_tree:.2f}\")\n",
    "print(f\"MAE: {mae_tree:.2f}\")\n",
    "print(f\"R^2: {r2_tree:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Категориальные признаки кодируются при помощи OneHotEncoder и объединяются с числовыми столбцами в общую таблицу признаков. Затем числовые признаки масштабируются через StandardScaler, чтобы дерево решений работало на более однородных по масштабу данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_onehot = OneHotEncoder(\n",
    "    sparse_output=False,\n",
    "    drop=\"first\",\n",
    "    handle_unknown=\"ignore\",\n",
    ")\n",
    "\n",
    "reg_cat_train_ohe = reg_onehot.fit_transform(X_reg_train[reg_categorical_cols])\n",
    "reg_cat_test_ohe = reg_onehot.transform(X_reg_test[reg_categorical_cols])\n",
    "\n",
    "reg_ohe_cols = reg_onehot.get_feature_names_out(reg_categorical_cols)\n",
    "\n",
    "X_reg_train_ohe = pd.concat(\n",
    "    [\n",
    "        X_reg_train[reg_numeric_cols].reset_index(drop=True),\n",
    "        pd.DataFrame(reg_cat_train_ohe, columns=reg_ohe_cols),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "X_reg_test_ohe = pd.concat(\n",
    "    [\n",
    "        X_reg_test[reg_numeric_cols].reset_index(drop=True),\n",
    "        pd.DataFrame(reg_cat_test_ohe, columns=reg_ohe_cols),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "reg_scaler = StandardScaler()\n",
    "X_reg_train_scaled = X_reg_train_ohe.copy()\n",
    "X_reg_test_scaled = X_reg_test_ohe.copy()\n",
    "\n",
    "X_reg_train_scaled[reg_numeric_cols] = reg_scaler.fit_transform(\n",
    "    X_reg_train_ohe[reg_numeric_cols]\n",
    ")\n",
    "X_reg_test_scaled[reg_numeric_cols] = reg_scaler.transform(\n",
    "    X_reg_test_ohe[reg_numeric_cols]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проводится кросс-валидация модели дерева решений на улучшенных признаках: после one-hot кодирования и масштабирования, с ограничением глубины до 5 уровней. Затем эта модель обучается на всём train-наборе, тестируется на отложенных данных и выводятся обновлённые значения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Регрессия: улучшенный бейзлайн (train CV) ===\n",
      "MSE_mean: 279004099.25\n",
      "MAE_mean: 11541.22\n",
      "R2_mean:  0.878\n",
      "\n",
      "Регрессия - улучшенный бейзлайн (test)\n",
      "--------------------------------------\n",
      "MSE: 319154446.65\n",
      "MAE: 10917.80\n",
      "R^2: 0.867\n"
     ]
    }
   ],
   "source": [
    "reg_tree_imp_mean, reg_tree_imp_std = regression_cross_validate(\n",
    "    DecisionTreeRegressor,\n",
    "    X_reg_train_scaled.to_numpy(),\n",
    "    y_reg_train.to_numpy(),\n",
    "    n_folds=5,\n",
    "    max_depth=5,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"=== Регрессия: улучшенный бейзлайн (train CV) ===\")\n",
    "print(f\"MSE_mean: {reg_tree_imp_mean['MSE']:.2f}\")\n",
    "print(f\"MAE_mean: {reg_tree_imp_mean['MAE']:.2f}\")\n",
    "print(f\"R2_mean:  {reg_tree_imp_mean['R2']:.3f}\")\n",
    "\n",
    "dt_improved = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "dt_improved.fit(X_reg_train_scaled, y_reg_train)\n",
    "\n",
    "y_reg_pred_imp = dt_improved.predict(X_reg_test_scaled)\n",
    "\n",
    "mse_imp = mean_squared_error(y_reg_test, y_reg_pred_imp)\n",
    "mae_imp = mean_absolute_error(y_reg_test, y_reg_pred_imp)\n",
    "r2_imp = r2_score(y_reg_test, y_reg_pred_imp)\n",
    "\n",
    "print(\"\\nРегрессия - улучшенный бейзлайн (test)\")\n",
    "print(\"--------------------------------------\")\n",
    "print(f\"MSE: {mse_imp:.2f}\")\n",
    "print(f\"MAE: {mae_imp:.2f}\")\n",
    "print(f\"R^2: {r2_imp:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Реализация своего класса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуется собственный регрессор на основе решающего дерева, который рекурсивно строит дерево разбиений по признакам, минимизируя сумму дисперсий в дочерних узлах. В методе predict каждый объект проходит по узлам дерева в зависимости от порогов признаков, пока не достигнет листа с предсказанным средним значением целевой переменной"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecisionTreeRegressor:\n",
    "    def __init__(self, max_depth: int | None = None):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        y = np.asarray(y, dtype=float)\n",
    "        self.tree = self._build_tree(X, y, depth=0)\n",
    "        return self\n",
    "\n",
    "    def _build_tree(self, X, y, depth: int):\n",
    "        if len(np.unique(y)) == 1 or (self.max_depth is not None and depth >= self.max_depth):\n",
    "            return float(np.mean(y))\n",
    "\n",
    "        best_split = self._find_best_split(X, y)\n",
    "        if best_split is None:\n",
    "            return float(np.mean(y))\n",
    "\n",
    "        feature_idx = best_split[\"feature\"]\n",
    "        threshold = best_split[\"value\"]\n",
    "\n",
    "        left_mask = X[:, feature_idx] <= threshold\n",
    "        right_mask = ~left_mask\n",
    "\n",
    "        left_subtree = self._build_tree(X[left_mask], y[left_mask], depth + 1)\n",
    "        right_subtree = self._build_tree(X[right_mask], y[right_mask], depth + 1)\n",
    "\n",
    "        return {\n",
    "            \"feature\": feature_idx,\n",
    "            \"value\": threshold,\n",
    "            \"left\": left_subtree,\n",
    "            \"right\": right_subtree,\n",
    "        }\n",
    "\n",
    "    def _find_best_split(self, X, y):\n",
    "        best_split = None\n",
    "        best_score = float(\"inf\")\n",
    "\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        for feature in range(n_features):\n",
    "            values = np.unique(X[:, feature])\n",
    "\n",
    "            for val in values:\n",
    "                left_mask = X[:, feature] <= val\n",
    "                right_mask = ~left_mask\n",
    "\n",
    "                if left_mask.sum() == 0 or right_mask.sum() == 0:\n",
    "                    continue\n",
    "\n",
    "                left_y = y[left_mask]\n",
    "                right_y = y[right_mask]\n",
    "\n",
    "                score = self._calculate_split_score(left_y, right_y)\n",
    "\n",
    "                if score < best_score:\n",
    "                    best_score = score\n",
    "                    best_split = {\"feature\": feature, \"value\": val}\n",
    "\n",
    "        return best_split\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_split_score(left_y, right_y):\n",
    "        left_score = np.var(left_y) * len(left_y)\n",
    "        right_score = np.var(right_y) * len(right_y)\n",
    "        return left_score + right_score\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        return np.array([self._predict_sample(x, self.tree) for x in X])\n",
    "\n",
    "    def _predict_sample(self, x, node):\n",
    "        if isinstance(node, dict):\n",
    "            if x[node[\"feature\"]] <= node[\"value\"]:\n",
    "                return self._predict_sample(x, node[\"left\"])\n",
    "            else:\n",
    "                return self._predict_sample(x, node[\"right\"])\n",
    "        return node\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проводится k-fold кросс-валидация собственной реализации дерева решений на базовых признаках, после чего выводятся усреднённые метрики качества. Далее самописная модель обучается на train-выборке, тестируется на отложенных данных и по результатам считаются метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Регрессия: собственная модель (train CV, без улучшений) ===\n",
      "MSE_mean: 359695139.41\n",
      "MAE_mean: 11405.86\n",
      "R2_mean:  0.841\n",
      "\n",
      "Регрессия - собственная реализация (test, без улучшений)\n",
      "--------------------------------------------------------\n",
      "MSE: 260434901.74\n",
      "MAE: 9961.90\n",
      "R^2: 0.891\n"
     ]
    }
   ],
   "source": [
    "reg_my_base_mean, reg_my_base_std = regression_cross_validate(\n",
    "    MyDecisionTreeRegressor,\n",
    "    X_reg_train_base.to_numpy(),\n",
    "    y_reg_train.to_numpy(),\n",
    "    n_folds=5,\n",
    ")\n",
    "\n",
    "print(\"=== Регрессия: собственная модель (train CV, без улучшений) ===\")\n",
    "print(f\"MSE_mean: {reg_my_base_mean['MSE']:.2f}\")\n",
    "print(f\"MAE_mean: {reg_my_base_mean['MAE']:.2f}\")\n",
    "print(f\"R2_mean:  {reg_my_base_mean['R2']:.3f}\")\n",
    "\n",
    "my_tree_base = MyDecisionTreeRegressor()\n",
    "my_tree_base.fit(X_reg_train_base.to_numpy(), y_reg_train.to_numpy())\n",
    "\n",
    "y_reg_pred_my_base = my_tree_base.predict(X_reg_test_base.to_numpy())\n",
    "\n",
    "mse_my_base = mean_squared_error(y_reg_test, y_reg_pred_my_base)\n",
    "mae_my_base = mean_absolute_error(y_reg_test, y_reg_pred_my_base)\n",
    "r2_my_base = r2_score(y_reg_test, y_reg_pred_my_base)\n",
    "\n",
    "print(\"\\nРегрессия - собственная реализация (test, без улучшений)\")\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(f\"MSE: {mse_my_base:.2f}\")\n",
    "print(f\"MAE: {mae_my_base:.2f}\")\n",
    "print(f\"R^2: {r2_my_base:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь та же самописная модель дерева решений обучается уже на улучшенных признаках: после one-hot кодирования, масштабирования и с ограничением глубины. Кросс-валидация и тест на отложенной выборке позволяют сравнить, как изменение признаков и параметра max_depth влияет на MSE, MAE и R^2 по сравнению с базовым вариантом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Регрессия: собственная реализация (train CV, с улучшениями) ===\n",
      "MSE_mean: 304992473.79\n",
      "MAE_mean: 11863.85\n",
      "R2_mean:  0.866\n",
      "\n",
      "Регрессия — собственная реализация (test, с улучшениями)\n",
      "--------------------------------------------------------\n",
      "MSE: 329006298.51\n",
      "MAE: 11228.91\n",
      "R^2: 0.863\n"
     ]
    }
   ],
   "source": [
    "reg_my_imp_mean, reg_my_imp_std = regression_cross_validate(\n",
    "    MyDecisionTreeRegressor,\n",
    "    X_reg_train_scaled.to_numpy(),\n",
    "    y_reg_train.to_numpy(),\n",
    "    n_folds=5,\n",
    "    max_depth=5,\n",
    ")\n",
    "\n",
    "print(\"\\n=== Регрессия: собственная реализация (train CV, с улучшениями) ===\")\n",
    "print(f\"MSE_mean: {reg_my_imp_mean['MSE']:.2f}\")\n",
    "print(f\"MAE_mean: {reg_my_imp_mean['MAE']:.2f}\")\n",
    "print(f\"R2_mean:  {reg_my_imp_mean['R2']:.3f}\")\n",
    "\n",
    "my_tree_improved = MyDecisionTreeRegressor(max_depth=5)\n",
    "my_tree_improved.fit(X_reg_train_scaled.to_numpy(), y_reg_train.to_numpy())\n",
    "\n",
    "y_reg_pred_my_imp = my_tree_improved.predict(X_reg_test_scaled.to_numpy())\n",
    "\n",
    "mse_my_imp = mean_squared_error(y_reg_test, y_reg_pred_my_imp)\n",
    "mae_my_imp = mean_absolute_error(y_reg_test, y_reg_pred_my_imp)\n",
    "r2_my_imp = r2_score(y_reg_test, y_reg_pred_my_imp)\n",
    "\n",
    "print(\"\\nРегрессия — собственная реализация (test, с улучшениями)\")\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(f\"MSE: {mse_my_imp:.2f}\")\n",
    "print(f\"MAE: {mae_my_imp:.2f}\")\n",
    "print(f\"R^2: {r2_my_imp:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 2. Классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>City</th>\n",
       "      <th>Profession</th>\n",
       "      <th>Academic Pressure</th>\n",
       "      <th>Work Pressure</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Study Satisfaction</th>\n",
       "      <th>Job Satisfaction</th>\n",
       "      <th>Sleep Duration</th>\n",
       "      <th>Dietary Habits</th>\n",
       "      <th>Degree</th>\n",
       "      <th>Have you ever had suicidal thoughts ?</th>\n",
       "      <th>Work/Study Hours</th>\n",
       "      <th>Financial Stress</th>\n",
       "      <th>Family History of Mental Illness</th>\n",
       "      <th>Depression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Male</td>\n",
       "      <td>33.0</td>\n",
       "      <td>Visakhapatnam</td>\n",
       "      <td>Student</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.97</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5-6 hours</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>B.Pharm</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>Female</td>\n",
       "      <td>24.0</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Student</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.90</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5-6 hours</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>BSc</td>\n",
       "      <td>No</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>Male</td>\n",
       "      <td>31.0</td>\n",
       "      <td>Srinagar</td>\n",
       "      <td>Student</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.03</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Less than 5 hours</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>BA</td>\n",
       "      <td>No</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>Female</td>\n",
       "      <td>28.0</td>\n",
       "      <td>Varanasi</td>\n",
       "      <td>Student</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.59</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7-8 hours</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>BCA</td>\n",
       "      <td>Yes</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>Female</td>\n",
       "      <td>25.0</td>\n",
       "      <td>Jaipur</td>\n",
       "      <td>Student</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.13</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5-6 hours</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>M.Tech</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  Gender   Age           City Profession  Academic Pressure  \\\n",
       "0   2    Male  33.0  Visakhapatnam    Student                5.0   \n",
       "1   8  Female  24.0      Bangalore    Student                2.0   \n",
       "2  26    Male  31.0       Srinagar    Student                3.0   \n",
       "3  30  Female  28.0       Varanasi    Student                3.0   \n",
       "4  32  Female  25.0         Jaipur    Student                4.0   \n",
       "\n",
       "   Work Pressure  CGPA  Study Satisfaction  Job Satisfaction  \\\n",
       "0            0.0  8.97                 2.0               0.0   \n",
       "1            0.0  5.90                 5.0               0.0   \n",
       "2            0.0  7.03                 5.0               0.0   \n",
       "3            0.0  5.59                 2.0               0.0   \n",
       "4            0.0  8.13                 3.0               0.0   \n",
       "\n",
       "      Sleep Duration Dietary Habits   Degree  \\\n",
       "0          5-6 hours        Healthy  B.Pharm   \n",
       "1          5-6 hours       Moderate      BSc   \n",
       "2  Less than 5 hours        Healthy       BA   \n",
       "3          7-8 hours       Moderate      BCA   \n",
       "4          5-6 hours       Moderate   M.Tech   \n",
       "\n",
       "  Have you ever had suicidal thoughts ?  Work/Study Hours  Financial Stress  \\\n",
       "0                                   Yes               3.0               1.0   \n",
       "1                                    No               3.0               2.0   \n",
       "2                                    No               9.0               1.0   \n",
       "3                                   Yes               4.0               5.0   \n",
       "4                                   Yes               1.0               1.0   \n",
       "\n",
       "  Family History of Mental Illness  Depression  \n",
       "0                               No           1  \n",
       "1                              Yes           0  \n",
       "2                              Yes           0  \n",
       "3                              Yes           1  \n",
       "4                               No           0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_df = pd.read_csv('data/Student Depression Dataset.csv')\n",
    "clf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполняется разбиение данных по депрессии на обучающую и тестовую выборки с сохранением исходного распределения классов. Затем для всех признаков заполняются пропуски наиболее частыми значениями, чтобы подготовить данные к дальнейшему кодированию и обучению дерева решений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_df = clf_df.copy()\n",
    "\n",
    "X_clf = clf_df.drop(columns=[\"Depression\", \"id\"])\n",
    "y_clf = clf_df[\"Depression\"]\n",
    "\n",
    "X_clf_train, X_clf_test, y_clf_train, y_clf_test = train_test_split(\n",
    "    X_clf,\n",
    "    y_clf,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_clf,\n",
    ")\n",
    "\n",
    "clf_categorical_cols = [\n",
    "    \"Gender\",\n",
    "    \"City\",\n",
    "    \"Profession\",\n",
    "    \"Sleep Duration\",\n",
    "    \"Dietary Habits\",\n",
    "    \"Degree\",\n",
    "    \"Have you ever had suicidal thoughts ?\",\n",
    "    \"Family History of Mental Illness\",\n",
    "]\n",
    "\n",
    "clf_numeric_cols = [\n",
    "    \"Age\",\n",
    "    \"Academic Pressure\",\n",
    "    \"Work Pressure\",\n",
    "    \"CGPA\",\n",
    "    \"Study Satisfaction\",\n",
    "    \"Job Satisfaction\",\n",
    "    \"Work/Study Hours\",\n",
    "    \"Financial Stress\",\n",
    "]\n",
    "\n",
    "clf_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "X_clf_train_imp = pd.DataFrame(\n",
    "    clf_imputer.fit_transform(X_clf_train),\n",
    "    columns=X_clf_train.columns,\n",
    ")\n",
    "X_clf_test_imp = pd.DataFrame(\n",
    "    clf_imputer.transform(X_clf_test),\n",
    "    columns=X_clf_test.columns,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Категориальные признаки кодируются с помощью OrdinalEncoder, при этом неизвестные категории явно отправляются в специальное значение 99. Далее оценивается базовое дерево решений по кросс-валидации и на тестовой выборке, чтобы получить отправную точку по точности, полноте и F1-мере для задачи классификации депрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN в X_clf_train_base: 0\n",
      "NaN в X_clf_test_base: 0\n",
      "\n",
      "=== Классификация: бейзлайн (train CV) ===\n",
      "Accuracy_mean:  0.767\n",
      "Precision_mean:0.768\n",
      "Recall_mean:   0.767\n",
      "F1_mean:       0.767\n",
      "\n",
      "Классификация - бейзлайн (test)\n",
      "--------------------------------\n",
      "1. Accuracy:  76.44%\n",
      "2. Precision: 76.49%\n",
      "3. Recall:    76.44%\n",
      "4. F1-score:  76.46%\n"
     ]
    }
   ],
   "source": [
    "clf_ordinal_encoder = OrdinalEncoder(\n",
    "    handle_unknown=\"use_encoded_value\",\n",
    "    unknown_value=99,\n",
    ")\n",
    "\n",
    "X_clf_train_base = X_clf_train_imp.copy()\n",
    "X_clf_test_base = X_clf_test_imp.copy()\n",
    "\n",
    "X_clf_train_base[clf_categorical_cols] = clf_ordinal_encoder.fit_transform(\n",
    "    X_clf_train_base[clf_categorical_cols]\n",
    ")\n",
    "X_clf_test_base[clf_categorical_cols] = clf_ordinal_encoder.transform(\n",
    "    X_clf_test_base[clf_categorical_cols]\n",
    ")\n",
    "\n",
    "print(\"NaN в X_clf_train_base:\", X_clf_train_base.isna().sum().sum())\n",
    "print(\"NaN в X_clf_test_base:\", X_clf_test_base.isna().sum().sum())\n",
    "\n",
    "clf_tree_base_mean, clf_tree_base_std = classification_cross_validate(\n",
    "    DecisionTreeClassifier,\n",
    "    X_clf_train_base.to_numpy(),\n",
    "    y_clf_train.to_numpy(),\n",
    "    n_folds=5,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"\\n=== Классификация: бейзлайн (train CV) ===\")\n",
    "print(f\"Accuracy_mean:  {clf_tree_base_mean['Accuracy']:.3f}\")\n",
    "print(f\"Precision_mean:{clf_tree_base_mean['Precision']:.3f}\")\n",
    "print(f\"Recall_mean:   {clf_tree_base_mean['Recall']:.3f}\")\n",
    "print(f\"F1_mean:       {clf_tree_base_mean['F1-score']:.3f}\")\n",
    "\n",
    "clf_tree_baseline = DecisionTreeClassifier(random_state=42)\n",
    "clf_tree_baseline.fit(X_clf_train_base, y_clf_train)\n",
    "\n",
    "y_clf_pred_base = clf_tree_baseline.predict(X_clf_test_base)\n",
    "\n",
    "acc_base = accuracy_score(y_clf_test, y_clf_pred_base)\n",
    "prec_base = precision_score(y_clf_test, y_clf_pred_base, average=\"weighted\", zero_division=0)\n",
    "rec_base = recall_score(y_clf_test, y_clf_pred_base, average=\"weighted\", zero_division=0)\n",
    "f1_base = f1_score(y_clf_test, y_clf_pred_base, average=\"weighted\", zero_division=0)\n",
    "\n",
    "print(\"\\nКлассификация - бейзлайн (test)\")\n",
    "print(\"--------------------------------\")\n",
    "print(f\"1. Accuracy:  {acc_base:.2%}\")\n",
    "print(f\"2. Precision: {prec_base:.2%}\")\n",
    "print(f\"3. Recall:    {rec_base:.2%}\")\n",
    "print(f\"4. F1-score:  {f1_base:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_onehot = OneHotEncoder(\n",
    "    sparse_output=False,\n",
    "    drop=\"first\",\n",
    "    handle_unknown=\"ignore\",\n",
    ")\n",
    "\n",
    "train_cat_ohe = clf_onehot.fit_transform(X_clf_train_imp[clf_categorical_cols])\n",
    "test_cat_ohe = clf_onehot.transform(X_clf_test_imp[clf_categorical_cols])\n",
    "\n",
    "ohe_clf_cols = clf_onehot.get_feature_names_out(clf_categorical_cols)\n",
    "\n",
    "X_clf_train_ohe = pd.concat(\n",
    "    [\n",
    "        X_clf_train_imp[clf_numeric_cols].reset_index(drop=True),\n",
    "        pd.DataFrame(train_cat_ohe, columns=ohe_clf_cols),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "X_clf_test_ohe = pd.concat(\n",
    "    [\n",
    "        X_clf_test_imp[clf_numeric_cols].reset_index(drop=True),\n",
    "        pd.DataFrame(test_cat_ohe, columns=ohe_clf_cols),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "clf_scaler = StandardScaler()\n",
    "X_clf_train_upd = X_clf_train_ohe.copy()\n",
    "X_clf_test_upd = X_clf_test_ohe.copy()\n",
    "\n",
    "X_clf_train_upd[clf_numeric_cols] = clf_scaler.fit_transform(\n",
    "    X_clf_train_ohe[clf_numeric_cols]\n",
    ")\n",
    "X_clf_test_upd[clf_numeric_cols] = clf_scaler.transform(\n",
    "    X_clf_test_ohe[clf_numeric_cols]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для улучшенного дерева решений используем расширенный препроцессинг (One-Hot Encoding + масштабирование числовых признаков) и ограничиваем глубину дерева, чтобы снизить переобучение. Затем оцениваем такую модель с помощью кросс-валидации и на тестовой выборке, сравнивая прирост по Accuracy, Precision, Recall и F1 относительно базовой версии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Классификация: улучшенный бейзлайн (train CV) ===\n",
      "Accuracy_mean:  0.825\n",
      "Precision_mean:0.825\n",
      "Recall_mean:   0.825\n",
      "F1_mean:       0.824\n",
      "\n",
      "Классификация - улучшенный бейзлайн (test)\n",
      "------------------------------------------\n",
      "1. Accuracy:  82.06%\n",
      "2. Precision: 81.99%\n",
      "3. Recall:    82.06%\n",
      "4. F1-score:  81.99%\n"
     ]
    }
   ],
   "source": [
    "clf_tree_imp_mean, clf_tree_imp_std = classification_cross_validate(\n",
    "    DecisionTreeClassifier,\n",
    "    X_clf_train_upd.to_numpy(),\n",
    "    y_clf_train.to_numpy(),\n",
    "    n_folds=5,\n",
    "    max_depth=5,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"=== Классификация: улучшенный бейзлайн (train CV) ===\")\n",
    "print(f\"Accuracy_mean:  {clf_tree_imp_mean['Accuracy']:.3f}\")\n",
    "print(f\"Precision_mean:{clf_tree_imp_mean['Precision']:.3f}\")\n",
    "print(f\"Recall_mean:   {clf_tree_imp_mean['Recall']:.3f}\")\n",
    "print(f\"F1_mean:       {clf_tree_imp_mean['F1-score']:.3f}\")\n",
    "\n",
    "clf_tree_improved = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "clf_tree_improved.fit(X_clf_train_upd, y_clf_train)\n",
    "\n",
    "y_clf_pred_imp = clf_tree_improved.predict(X_clf_test_upd)\n",
    "\n",
    "acc_imp = accuracy_score(y_clf_test, y_clf_pred_imp)\n",
    "prec_imp = precision_score(y_clf_test, y_clf_pred_imp, average=\"weighted\", zero_division=0)\n",
    "rec_imp = recall_score(y_clf_test, y_clf_pred_imp, average=\"weighted\", zero_division=0)\n",
    "f1_imp = f1_score(y_clf_test, y_clf_pred_imp, average=\"weighted\", zero_division=0)\n",
    "\n",
    "print(\"\\nКлассификация - улучшенный бейзлайн (test)\")\n",
    "print(\"------------------------------------------\")\n",
    "print(f\"1. Accuracy:  {acc_imp:.2%}\")\n",
    "print(f\"2. Precision: {prec_imp:.2%}\")\n",
    "print(f\"3. Recall:    {rec_imp:.2%}\")\n",
    "print(f\"4. F1-score:  {f1_imp:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем собственный классификатор на основе дерева решений, который строит бинарное дерево, выбирая разбиения по максимуму информационного выигрыша. Для предсказания новый объект спускается по ветвям дерева в зависимости от порогов признаков, пока не достигнет листа с меткой класса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecisionTreeClassifier:\n",
    "    def __init__(self, max_depth: int | None = None):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        y = np.asarray(y)\n",
    "        self.tree_ = self._build_tree(X, y, depth=0)\n",
    "        return self\n",
    "\n",
    "    def _build_tree(self, X, y, depth: int):\n",
    "        n_samples, n_features = X.shape\n",
    "        classes = np.unique(y)\n",
    "\n",
    "        if len(classes) == 1:\n",
    "            return classes[0]\n",
    "        if n_samples <= 1:\n",
    "            return self._most_common_class(y)\n",
    "        if self.max_depth is not None and depth >= self.max_depth:\n",
    "            return self._most_common_class(y)\n",
    "\n",
    "        best_split = self._find_best_split(X, y)\n",
    "        if best_split is None:\n",
    "            return self._most_common_class(y)\n",
    "\n",
    "        left_mask = best_split[\"left_mask\"]\n",
    "        right_mask = best_split[\"right_mask\"]\n",
    "\n",
    "        left_subtree = self._build_tree(X[left_mask], y[left_mask], depth + 1)\n",
    "        right_subtree = self._build_tree(X[right_mask], y[right_mask], depth + 1)\n",
    "\n",
    "        return {\n",
    "            \"feature_idx\": best_split[\"feature_idx\"],\n",
    "            \"threshold\": best_split[\"threshold\"],\n",
    "            \"left\": left_subtree,\n",
    "            \"right\": right_subtree,\n",
    "        }\n",
    "\n",
    "    def _find_best_split(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        best_gain = -np.inf\n",
    "        best_split = None\n",
    "\n",
    "        for feature_idx in range(n_features):\n",
    "            feature_values = X[:, feature_idx]\n",
    "            unique_vals = np.unique(feature_values)\n",
    "\n",
    "            for thr in unique_vals:\n",
    "                left_mask = feature_values <= thr\n",
    "                right_mask = feature_values > thr\n",
    "\n",
    "                if left_mask.sum() == 0 or right_mask.sum() == 0:\n",
    "                    continue\n",
    "\n",
    "                gain = self._information_gain(y, left_mask, right_mask)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_split = {\n",
    "                        \"feature_idx\": feature_idx,\n",
    "                        \"threshold\": thr,\n",
    "                        \"left_mask\": left_mask,\n",
    "                        \"right_mask\": right_mask,\n",
    "                    }\n",
    "\n",
    "        return best_split\n",
    "\n",
    "    def _information_gain(self, y, left_mask, right_mask):\n",
    "        y_left = y[left_mask]\n",
    "        y_right = y[right_mask]\n",
    "\n",
    "        H_parent = self._entropy(y)\n",
    "        H_left = self._entropy(y_left)\n",
    "        H_right = self._entropy(y_right)\n",
    "\n",
    "        w_left = len(y_left) / len(y)\n",
    "        w_right = len(y_right) / len(y)\n",
    "\n",
    "        return H_parent - (w_left * H_left + w_right * H_right)\n",
    "\n",
    "    @staticmethod\n",
    "    def _entropy(y):\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        probs = counts / counts.sum()\n",
    "        return -np.sum(probs * np.log2(probs + 1e-9))\n",
    "\n",
    "    @staticmethod\n",
    "    def _most_common_class(y):\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        return classes[np.argmax(counts)]\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        return np.array([self._predict_single(x, self.tree_) for x in X])\n",
    "\n",
    "    def _predict_single(self, x, node):\n",
    "        if not isinstance(node, dict):\n",
    "            return node\n",
    "\n",
    "        feat_idx = node[\"feature_idx\"]\n",
    "        thr = node[\"threshold\"]\n",
    "\n",
    "        if x[feat_idx] <= thr:\n",
    "            return self._predict_single(x, node[\"left\"])\n",
    "        else:\n",
    "            return self._predict_single(x, node[\"right\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для собственной реализации дерева решений проводим K-fold кросс-валидацию и оцениваем средние показатели Accuracy, Precision, Recall и F1 на обучающей выборке. Затем обучаем модель на train-данных и считаем те же метрики на тесте, чтобы сравнить качество с бейзлайном из sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Классификация: собственная модель (train CV, без улучшений) ===\n",
      "Accuracy_mean:  0.772\n",
      "Precision_mean:0.773\n",
      "Recall_mean:   0.772\n",
      "F1_mean:       0.772\n",
      "\n",
      "Классификация - собственная реализация (test, без улучшений)\n",
      "------------------------------------------------------------\n",
      "1. Accuracy:  76.80%\n",
      "2. Precision: 76.80%\n",
      "3. Recall:    76.80%\n",
      "4. F1-score:  76.80%\n"
     ]
    }
   ],
   "source": [
    "clf_my_base_mean, clf_my_base_std = classification_cross_validate(\n",
    "    MyDecisionTreeClassifier,\n",
    "    X_clf_train_base.to_numpy(),\n",
    "    y_clf_train.to_numpy(),\n",
    "    n_folds=5,\n",
    ")\n",
    "\n",
    "print(\"=== Классификация: собственная модель (train CV, без улучшений) ===\")\n",
    "print(f\"Accuracy_mean:  {clf_my_base_mean['Accuracy']:.3f}\")\n",
    "print(f\"Precision_mean:{clf_my_base_mean['Precision']:.3f}\")\n",
    "print(f\"Recall_mean:   {clf_my_base_mean['Recall']:.3f}\")\n",
    "print(f\"F1_mean:       {clf_my_base_mean['F1-score']:.3f}\")\n",
    "\n",
    "my_tree_base = MyDecisionTreeClassifier()\n",
    "my_tree_base.fit(X_clf_train_base.to_numpy(), y_clf_train.to_numpy())\n",
    "\n",
    "y_clf_pred_my_base = my_tree_base.predict(X_clf_test_base.to_numpy())\n",
    "\n",
    "acc_my_base = accuracy_score(y_clf_test, y_clf_pred_my_base)\n",
    "prec_my_base = precision_score(y_clf_test, y_clf_pred_my_base, average=\"weighted\", zero_division=0)\n",
    "rec_my_base = recall_score(y_clf_test, y_clf_pred_my_base, average=\"weighted\", zero_division=0)\n",
    "f1_my_base = f1_score(y_clf_test, y_clf_pred_my_base, average=\"weighted\", zero_division=0)\n",
    "\n",
    "print(\"\\nКлассификация - собственная реализация (test, без улучшений)\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(f\"1. Accuracy:  {acc_my_base:.2%}\")\n",
    "print(f\"2. Precision: {prec_my_base:.2%}\")\n",
    "print(f\"3. Recall:    {rec_my_base:.2%}\")\n",
    "print(f\"4. F1-score:  {f1_my_base:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для улучшенной версии собственного дерева решений снова проводим кросс-валидацию, но уже на данных после OneHotEncoding, масштабирования и с ограничением глубины. Затем обучаем модель с `max_depth=5` на этих признаках и считаем Accuracy, Precision, Recall и F1 на тесте, чтобы увидеть, как предобработка и настройка глубины влияют на качество классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Классификация: собственная модель (train CV, с улучшениями) ===\n",
      "Accuracy_mean:  0.825\n",
      "Precision_mean:0.825\n",
      "Recall_mean:   0.825\n",
      "F1_mean:       0.825\n",
      "\n",
      "Классификация - собственная реализация (test, с улучшениями)\n",
      "------------------------------------------------------------\n",
      "1. Accuracy:  82.06%\n",
      "2. Precision: 81.99%\n",
      "3. Recall:    82.06%\n",
      "4. F1-score:  81.99%\n"
     ]
    }
   ],
   "source": [
    "clf_my_imp_mean, clf_my_imp_std = classification_cross_validate(\n",
    "    MyDecisionTreeClassifier,\n",
    "    X_clf_train_upd.to_numpy(),\n",
    "    y_clf_train.to_numpy(),\n",
    "    n_folds=5,\n",
    "    max_depth=5,\n",
    ")\n",
    "\n",
    "print(\"=== Классификация: собственная модель (train CV, с улучшениями) ===\")\n",
    "print(f\"Accuracy_mean:  {clf_my_imp_mean['Accuracy']:.3f}\")\n",
    "print(f\"Precision_mean:{clf_my_imp_mean['Precision']:.3f}\")\n",
    "print(f\"Recall_mean:   {clf_my_imp_mean['Recall']:.3f}\")\n",
    "print(f\"F1_mean:       {clf_my_imp_mean['F1-score']:.3f}\")\n",
    "\n",
    "my_tree_improved = MyDecisionTreeClassifier(max_depth=5)\n",
    "my_tree_improved.fit(X_clf_train_upd.to_numpy(), y_clf_train.to_numpy())\n",
    "\n",
    "y_clf_pred_my_imp = my_tree_improved.predict(X_clf_test_upd.to_numpy())\n",
    "\n",
    "acc_my_imp = accuracy_score(y_clf_test, y_clf_pred_my_imp)\n",
    "prec_my_imp = precision_score(y_clf_test, y_clf_pred_my_imp, average=\"weighted\", zero_division=0)\n",
    "rec_my_imp = recall_score(y_clf_test, y_clf_pred_my_imp, average=\"weighted\", zero_division=0)\n",
    "f1_my_imp = f1_score(y_clf_test, y_clf_pred_my_imp, average=\"weighted\", zero_division=0)\n",
    "\n",
    "print(\"\\nКлассификация - собственная реализация (test, с улучшениями)\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(f\"1. Accuracy:  {acc_my_imp:.2%}\")\n",
    "print(f\"2. Precision: {prec_my_imp:.2%}\")\n",
    "print(f\"3. Recall:    {rec_my_imp:.2%}\")\n",
    "print(f\"4. F1-score:  {f1_my_imp:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заключение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для регрессии решающие деревья без особого тюнинга дают неплохой результат, но попытки «улучшить» модель (глубина + препроцессинг) местами приводят к лёгкому переобучению на тесте. Для классификации улучшенный препроцессинг и ограничение глубины стабильно поднимают качество, а самописные реализации по метрикам почти полностью повторяют поведение моделей `sklearn`.\n",
    "\n",
    "\n",
    "### Сводная таблица по регрессии (test)\n",
    "\n",
    "| Модель                               |    MSE   |    MAE   | R^2 |\n",
    "| :----------------------------------- | :------: | :------: | :---: |\n",
    "| Sklearn (до улучшения)               | 2.31e+08 |  9428.57 | 0.904 |\n",
    "| Sklearn (после улучшения)            | 3.19e+08 | 10917.80 | 0.867 |\n",
    "| Собственная модель (до улучшения)    | 2.60e+08 |  9961.90 | 0.891 |\n",
    "| Собственная модель (после улучшения) | 3.29e+08 | 11228.91 | 0.863 |\n",
    "\n",
    "\n",
    "### Сводная таблица по классификации (test)\n",
    "\n",
    "| Модель                               | Accuracy | Precision | Recall | F1-score |\n",
    "| :----------------------------------- | -------: | --------: | -----: | -------: |\n",
    "| Sklearn (до улучшения)               |   76.44% |    76.49% | 76.44% |   76.46% |\n",
    "| Sklearn (после улучшения)            |   82.06% |    81.99% | 82.06% |   81.99% |\n",
    "| Собственная модель (до улучшения)    |   76.80% |    76.80% | 76.80% |   76.80% |\n",
    "| Собственная модель (после улучшения) |   82.06% |    81.99% | 82.06% |   81.99% |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
