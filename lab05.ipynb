{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа 5 (Gradient Boosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Импорт всех необходимых модулей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import (\n",
    "    LabelEncoder,\n",
    "    StandardScaler,\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "\n",
    "\n",
    "def regression_cross_validate(model_cls, X, y, n_folds: int = 5, **model_kwargs):\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    mae_values = []\n",
    "    mse_values = []\n",
    "    r2_values = []\n",
    "\n",
    "    for fold_idx, (idx_train, idx_valid) in enumerate(kf.split(X), start=1):\n",
    "        X_train, X_valid = X[idx_train], X[idx_valid]\n",
    "        y_train, y_valid = y[idx_train], y[idx_valid]\n",
    "\n",
    "        model = model_cls(**model_kwargs)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = model.predict(X_valid)\n",
    "\n",
    "        mae_values.append(mean_absolute_error(y_valid, y_pred))\n",
    "        mse_values.append(mean_squared_error(y_valid, y_pred))\n",
    "        r2_values.append(r2_score(y_valid, y_pred))\n",
    "\n",
    "    metrics_avg = {\n",
    "        \"MAE\": float(np.mean(mae_values)),\n",
    "        \"MSE\": float(np.mean(mse_values)),\n",
    "        \"R2\": float(np.mean(r2_values)),\n",
    "    }\n",
    "\n",
    "    metrics_std = {\n",
    "        \"MAE\": float(np.std(mae_values)),\n",
    "        \"MSE\": float(np.std(mse_values)),\n",
    "        \"R2\": float(np.std(r2_values)),\n",
    "    }\n",
    "\n",
    "    return metrics_avg, metrics_std\n",
    "\n",
    "\n",
    "def classification_cross_validate(model_cls, X, y, n_folds: int = 5, **model_kwargs):\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    acc_list = []\n",
    "    prec_list = []\n",
    "    rec_list = []\n",
    "    f1_list = []\n",
    "\n",
    "    for fold_idx, (idx_train, idx_valid) in enumerate(kf.split(X), start=1):\n",
    "        X_train, X_valid = X[idx_train], X[idx_valid]\n",
    "        y_train, y_valid = y[idx_train], y[idx_valid]\n",
    "\n",
    "        model = model_cls(**model_kwargs)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = model.predict(X_valid)\n",
    "\n",
    "        acc_list.append(accuracy_score(y_valid, y_pred))\n",
    "        prec_list.append(precision_score(y_valid, y_pred, average=\"weighted\", zero_division=0))\n",
    "        rec_list.append(recall_score(y_valid, y_pred, average=\"weighted\", zero_division=0))\n",
    "        f1_list.append(f1_score(y_valid, y_pred, average=\"weighted\", zero_division=0))\n",
    "\n",
    "    metrics_avg = {\n",
    "        \"Accuracy\": float(np.mean(acc_list)),\n",
    "        \"Precision\": float(np.mean(prec_list)),\n",
    "        \"Recall\": float(np.mean(rec_list)),\n",
    "        \"F1-score\": float(np.mean(f1_list)),\n",
    "    }\n",
    "\n",
    "    metrics_std = {\n",
    "        \"Accuracy\": float(np.std(acc_list)),\n",
    "        \"Precision\": float(np.std(prec_list)),\n",
    "        \"Recall\": float(np.std(rec_list)),\n",
    "        \"F1-score\": float(np.std(f1_list)),\n",
    "    }\n",
    "\n",
    "    return metrics_avg, metrics_std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 1. Регрессия\n",
    "Данные о зарплатах загружаются в DataFrame, и выводятся первые строки, чтобы убедиться, что файл прочитан корректно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер датасета Salary_Data: (375, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Education Level</th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Years of Experience</th>\n",
       "      <th>Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>5.0</td>\n",
       "      <td>90000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Master's</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>3.0</td>\n",
       "      <td>65000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>PhD</td>\n",
       "      <td>Senior Manager</td>\n",
       "      <td>15.0</td>\n",
       "      <td>150000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Sales Associate</td>\n",
       "      <td>7.0</td>\n",
       "      <td>60000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Master's</td>\n",
       "      <td>Director</td>\n",
       "      <td>20.0</td>\n",
       "      <td>200000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age  Gender Education Level          Job Title  Years of Experience  \\\n",
       "0  32.0    Male      Bachelor's  Software Engineer                  5.0   \n",
       "1  28.0  Female        Master's       Data Analyst                  3.0   \n",
       "2  45.0    Male             PhD     Senior Manager                 15.0   \n",
       "3  36.0  Female      Bachelor's    Sales Associate                  7.0   \n",
       "4  52.0    Male        Master's           Director                 20.0   \n",
       "\n",
       "     Salary  \n",
       "0   90000.0  \n",
       "1   65000.0  \n",
       "2  150000.0  \n",
       "3   60000.0  \n",
       "4  200000.0  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salary_df = pd.read_csv(\"data/Salary Data.csv\")\n",
    "print(\"Размер датасета Salary_Data:\", salary_df.shape)\n",
    "\n",
    "salary_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка данных для регрессии\n",
    "\n",
    "Сначала из датасета убирается столбец `Job Title`: он содержит много уникальных значений и в таком виде мало помогает линейной модели, только раздувает пространство признаков\n",
    "\n",
    "Категориальные признаки (`Gender`, `Education Level`) переводятся в числовой вид с помощью `OrdinalEncoder`, чтобы их можно было использовать в линейной регрессии и в собственной реализации модели\n",
    "\n",
    "После этого все пропуски в данных заполняются наиболее частыми значениями (`SimpleImputer` с стратегией `\"most_frequent\"`). Такой шаг нужен, потому что большинство моделей из `sklearn`, а также наши собственные реализации, не умеют работать с `NaN` и ожидают полностью числовую матрицу признаков без пропусков\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN в X_train_base: 0\n",
      "NaN в X_test_base: 0\n"
     ]
    }
   ],
   "source": [
    "reg_df = salary_df.copy()\n",
    "reg_df = reg_df.dropna(subset=[\"Salary\"])\n",
    "\n",
    "if \"Job Title\" in reg_df.columns:\n",
    "    reg_df = reg_df.drop(columns=[\"Job Title\"])\n",
    "\n",
    "X_reg = reg_df.drop(columns=[\"Salary\"])\n",
    "y_reg = reg_df[\"Salary\"]\n",
    "\n",
    "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "reg_categorical_cols = [\"Gender\", \"Education Level\"]\n",
    "reg_numeric_cols = [col for col in X_reg.columns if col not in reg_categorical_cols]\n",
    "\n",
    "reg_ordinal_encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "\n",
    "X_reg_train_base = X_reg_train.copy()\n",
    "X_reg_test_base = X_reg_test.copy()\n",
    "\n",
    "X_reg_train_base[reg_categorical_cols] = reg_ordinal_encoder.fit_transform(\n",
    "    X_reg_train_base[reg_categorical_cols]\n",
    ")\n",
    "X_reg_test_base[reg_categorical_cols] = reg_ordinal_encoder.transform(\n",
    "    X_reg_test_base[reg_categorical_cols]\n",
    ")\n",
    "\n",
    "reg_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "X_reg_train_base = pd.DataFrame(\n",
    "    reg_imputer.fit_transform(X_reg_train_base),\n",
    "    columns=X_reg_train_base.columns\n",
    ")\n",
    "X_reg_test_base = pd.DataFrame(\n",
    "    reg_imputer.transform(X_reg_test_base),\n",
    "    columns=X_reg_test_base.columns\n",
    ")\n",
    "\n",
    "print(\"NaN в X_train_base:\", X_reg_train_base.isna().sum().sum())\n",
    "print(\"NaN в X_test_base:\", X_reg_test_base.isna().sum().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Построение бейзлайна"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь строится и оценивается базовая модель Gradient Boosting для задачи регрессии: сначала по K-fold кросс-валидации считаются средние значения MSE, MAE и \n",
    "R^2 на обучающей выборке. Затем та же конфигурация бустинга обучается на всём train-наборе и проверяется на отложенном тесте, что даёт отправную точку для дальнейшего сравнения с улучшёнными вариантами и собственными реализациями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Регрессия: Gradient Boosting (train CV, бейзлайн) ===\n",
      "MSE_mean: 269180690.32\n",
      "MAE_mean: 10778.25\n",
      "R2_mean:  0.881\n",
      "\n",
      "Регрессия — Gradient Boosting (test, бейзлайн)\n",
      "-----------------------------------------------\n",
      "MSE: 247801995.23\n",
      "MAE: 9997.82\n",
      "R^2: 0.897\n"
     ]
    }
   ],
   "source": [
    "gb_cv_mean, gb_cv_std = regression_cross_validate(\n",
    "    GradientBoostingRegressor,\n",
    "    X_reg_train_base.to_numpy(),\n",
    "    y_reg_train.to_numpy(),\n",
    "    n_folds=5,\n",
    "    random_state=42,\n",
    "    n_estimators=50,\n",
    "    max_depth=5,\n",
    ")\n",
    "\n",
    "print(\"=== Регрессия: Gradient Boosting (train CV, бейзлайн) ===\")\n",
    "print(f\"MSE_mean: {gb_cv_mean['MSE']:.2f}\")\n",
    "print(f\"MAE_mean: {gb_cv_mean['MAE']:.2f}\")\n",
    "print(f\"R2_mean:  {gb_cv_mean['R2']:.3f}\")\n",
    "\n",
    "gb_baseline = GradientBoostingRegressor(\n",
    "    n_estimators=50,\n",
    "    max_depth=5,\n",
    "    random_state=42,\n",
    ")\n",
    "gb_baseline.fit(X_reg_train_base, y_reg_train)\n",
    "\n",
    "y_reg_pred_gb = gb_baseline.predict(X_reg_test_base)\n",
    "\n",
    "mse_gb = mean_squared_error(y_reg_test, y_reg_pred_gb)\n",
    "mae_gb = mean_absolute_error(y_reg_test, y_reg_pred_gb)\n",
    "r2_gb = r2_score(y_reg_test, y_reg_pred_gb)\n",
    "\n",
    "print(\"\\nРегрессия — Gradient Boosting (test, бейзлайн)\")\n",
    "print(\"-----------------------------------------------\")\n",
    "print(f\"MSE: {mse_gb:.2f}\")\n",
    "print(f\"MAE: {mae_gb:.2f}\")\n",
    "print(f\"R^2: {r2_gb:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем выдвинутые гипотезы по улучшению признакового пространства: категориальные признаки переводятся в набор бинарных столбцов через one-hot кодирование, а численные - масштабируются с помощью StandardScaler. Полученные матрицы X_reg_train_gb и X_reg_test_gb комбинируют нормализованные числовые признаки и dummy-переменные, что делает данные более удобными для градиентного бустинга и позволяет модели лучше улавливать нелинейные зависимости\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_onehot = OneHotEncoder(\n",
    "    sparse_output=False,\n",
    "    drop=\"first\",\n",
    "    handle_unknown=\"ignore\",\n",
    ")\n",
    "\n",
    "gb_cat_train_ohe = gb_onehot.fit_transform(X_reg_train[reg_categorical_cols])\n",
    "gb_cat_test_ohe = gb_onehot.transform(X_reg_test[reg_categorical_cols])\n",
    "\n",
    "gb_ohe_cols = gb_onehot.get_feature_names_out(reg_categorical_cols)\n",
    "\n",
    "X_reg_train_gb = pd.concat(\n",
    "    [\n",
    "        X_reg_train[reg_numeric_cols].reset_index(drop=True),\n",
    "        pd.DataFrame(gb_cat_train_ohe, columns=gb_ohe_cols),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "X_reg_test_gb = pd.concat(\n",
    "    [\n",
    "        X_reg_test[reg_numeric_cols].reset_index(drop=True),\n",
    "        pd.DataFrame(gb_cat_test_ohe, columns=gb_ohe_cols),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "gb_scaler = StandardScaler()\n",
    "X_reg_train_gb[reg_numeric_cols] = gb_scaler.fit_transform(\n",
    "    X_reg_train_gb[reg_numeric_cols]\n",
    ")\n",
    "X_reg_test_gb[reg_numeric_cols] = gb_scaler.transform(\n",
    "    X_reg_test_gb[reg_numeric_cols]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь оценивается улучшенный вариант градиентного бустинга на обновлённых признаках: сначала по k-fold кросс-валидации вычисляются средние значения метрик на обучающей выборке. Затем модель с увеличенным числом деревьев и уменьшенной глубиной обучается на всём train-наборе и проверяется на тесте, что позволяет сравнить её качество с базовой конфигурацией бустинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Регрессия: Gradient Boosting (train CV, улучшенный бейзлайн) ===\n",
      "MSE_mean: 321934450.40\n",
      "MAE_mean: 10744.76\n",
      "R2_mean:  0.859\n",
      "\n",
      "Регрессия — Gradient Boosting (test, улучшенный бейзлайн)\n",
      "---------------------------------------------------------\n",
      "MSE: 220438940.57\n",
      "MAE: 9084.95\n",
      "R^2: 0.908\n"
     ]
    }
   ],
   "source": [
    "gb_imp_mean, gb_imp_std = regression_cross_validate(\n",
    "    GradientBoostingRegressor,\n",
    "    X_reg_train_gb.to_numpy(),\n",
    "    y_reg_train.to_numpy(),\n",
    "    n_folds=5,\n",
    "    random_state=42,\n",
    "    n_estimators=200,\n",
    "    max_depth=15,\n",
    ")\n",
    "\n",
    "print(\"=== Регрессия: Gradient Boosting (train CV, улучшенный бейзлайн) ===\")\n",
    "print(f\"MSE_mean: {gb_imp_mean['MSE']:.2f}\")\n",
    "print(f\"MAE_mean: {gb_imp_mean['MAE']:.2f}\")\n",
    "print(f\"R2_mean:  {gb_imp_mean['R2']:.3f}\")\n",
    "\n",
    "gb_improved = GradientBoostingRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=15,\n",
    "    random_state=42,\n",
    ")\n",
    "gb_improved.fit(X_reg_train_gb, y_reg_train)\n",
    "\n",
    "y_reg_pred_gb_imp = gb_improved.predict(X_reg_test_gb)\n",
    "\n",
    "mse_gb_imp = mean_squared_error(y_reg_test, y_reg_pred_gb_imp)\n",
    "mae_gb_imp = mean_absolute_error(y_reg_test, y_reg_pred_gb_imp)\n",
    "r2_gb_imp = r2_score(y_reg_test, y_reg_pred_gb_imp)\n",
    "\n",
    "print(\"\\nРегрессия — Gradient Boosting (test, улучшенный бейзлайн)\")\n",
    "print(\"---------------------------------------------------------\")\n",
    "print(f\"MSE: {mse_gb_imp:.2f}\")\n",
    "print(f\"MAE: {mae_gb_imp:.2f}\")\n",
    "print(f\"R^2: {r2_gb_imp:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Реализация своего класса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGradientBoostingRegressor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_estimators: int = 100,\n",
    "        learning_rate: float = 0.05,\n",
    "        max_depth: int = 3,\n",
    "    ):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "        self.trees: list[DecisionTreeRegressor] = []\n",
    "        self.initial_value: float | None = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        y = np.asarray(y, dtype=float)\n",
    "\n",
    "        self.initial_value = float(np.mean(y))\n",
    "        residual = y - self.initial_value\n",
    "        self.trees = []\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residual)\n",
    "            pred = tree.predict(X)\n",
    "            residual = residual - self.learning_rate * pred\n",
    "            self.trees.append(tree)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "\n",
    "        if self.initial_value is None:\n",
    "            raise RuntimeError(\"Сначала нужно вызвать fit().\")\n",
    "\n",
    "        y_pred = np.full(X.shape[0], self.initial_value, dtype=float)\n",
    "        for tree in self.trees:\n",
    "            y_pred = y_pred + self.learning_rate * tree.predict(X)\n",
    "\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь оценивается базовая версия собственной реализации градиентного бустинга на исходных закодированных признаках. Сначала по кросс-валидации проверяется устойчивость качества на обучающей выборке, затем та же конфигурация тестируется на отложенных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Регрессия: собственный Gradient Boosting (train CV, без улучшений) ===\n",
      "MSE_mean: 319429415.52\n",
      "MAE_mean: 11700.22\n",
      "R2_mean:  0.859\n",
      "\n",
      "Регрессия — собственный Gradient Boosting (test, без улучшений)\n",
      "----------------------------------------------------------------\n",
      "MSE: 231454926.07\n",
      "MAE: 10093.75\n",
      "R^2: 0.903\n"
     ]
    }
   ],
   "source": [
    "gb_my_base_mean, gb_my_base_std = regression_cross_validate(\n",
    "    MyGradientBoostingRegressor,\n",
    "    X_reg_train_base.to_numpy(),\n",
    "    y_reg_train.to_numpy(),\n",
    "    n_folds=5,\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    ")\n",
    "\n",
    "print(\"=== Регрессия: собственный Gradient Boosting (train CV, без улучшений) ===\")\n",
    "print(f\"MSE_mean: {gb_my_base_mean['MSE']:.2f}\")\n",
    "print(f\"MAE_mean: {gb_my_base_mean['MAE']:.2f}\")\n",
    "print(f\"R2_mean:  {gb_my_base_mean['R2']:.3f}\")\n",
    "\n",
    "my_gb_base = MyGradientBoostingRegressor(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    ")\n",
    "my_gb_base.fit(X_reg_train_base.to_numpy(), y_reg_train.to_numpy())\n",
    "\n",
    "y_reg_pred_my_gb_base = my_gb_base.predict(X_reg_test_base.to_numpy())\n",
    "\n",
    "mse_my_gb_base = mean_squared_error(y_reg_test, y_reg_pred_my_gb_base)\n",
    "mae_my_gb_base = mean_absolute_error(y_reg_test, y_reg_pred_my_gb_base)\n",
    "r2_my_gb_base = r2_score(y_reg_test, y_reg_pred_my_gb_base)\n",
    "\n",
    "print(\"\\nРегрессия — собственный Gradient Boosting (test, без улучшений)\")\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(f\"MSE: {mse_my_gb_base:.2f}\")\n",
    "print(f\"MAE: {mae_my_gb_base:.2f}\")\n",
    "print(f\"R^2: {r2_my_gb_base:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем, как собственная реализация градиентного бустинга ведёт себя на улучшенных признаках с one-hot кодированием и масштабированием. Сначала модель оценивается по кросс-валидации, затем обучается на всём тренировочном наборе и тестируется на отложенных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Регрессия: собственный Gradient Boosting (train CV, с улучшениями) ===\n",
      "MSE_mean: 320256670.36\n",
      "MAE_mean: 11057.95\n",
      "R2_mean:  0.859\n",
      "\n",
      "Регрессия — собственный Gradient Boosting (test, с улучшениями)\n",
      "----------------------------------------------------------------\n",
      "MSE: 214170660.81\n",
      "MAE: 9022.64\n",
      "R^2: 0.911\n"
     ]
    }
   ],
   "source": [
    "gb_my_imp_mean, gb_my_imp_std = regression_cross_validate(\n",
    "    MyGradientBoostingRegressor,\n",
    "    X_reg_train_gb.to_numpy(),\n",
    "    y_reg_train.to_numpy(),\n",
    "    n_folds=5,\n",
    "    n_estimators=500,\n",
    "    max_depth=8,\n",
    ")\n",
    "\n",
    "print(\"=== Регрессия: собственный Gradient Boosting (train CV, с улучшениями) ===\")\n",
    "print(f\"MSE_mean: {gb_my_imp_mean['MSE']:.2f}\")\n",
    "print(f\"MAE_mean: {gb_my_imp_mean['MAE']:.2f}\")\n",
    "print(f\"R2_mean:  {gb_my_imp_mean['R2']:.3f}\")\n",
    "\n",
    "my_gb_improved = MyGradientBoostingRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=8,\n",
    ")\n",
    "my_gb_improved.fit(X_reg_train_gb.to_numpy(), y_reg_train.to_numpy())\n",
    "\n",
    "y_reg_pred_my_gb_imp = my_gb_improved.predict(X_reg_test_gb.to_numpy())\n",
    "\n",
    "mse_my_gb_imp = mean_squared_error(y_reg_test, y_reg_pred_my_gb_imp)\n",
    "mae_my_gb_imp = mean_absolute_error(y_reg_test, y_reg_pred_my_gb_imp)\n",
    "r2_my_gb_imp = r2_score(y_reg_test, y_reg_pred_my_gb_imp)\n",
    "\n",
    "print(\"\\nРегрессия — собственный Gradient Boosting (test, с улучшениями)\")\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(f\"MSE: {mse_my_gb_imp:.2f}\")\n",
    "print(f\"MAE: {mae_my_gb_imp:.2f}\")\n",
    "print(f\"R^2: {r2_my_gb_imp:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 2. Классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Обработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>City</th>\n",
       "      <th>Profession</th>\n",
       "      <th>Academic Pressure</th>\n",
       "      <th>Work Pressure</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Study Satisfaction</th>\n",
       "      <th>Job Satisfaction</th>\n",
       "      <th>Sleep Duration</th>\n",
       "      <th>Dietary Habits</th>\n",
       "      <th>Degree</th>\n",
       "      <th>Have you ever had suicidal thoughts ?</th>\n",
       "      <th>Work/Study Hours</th>\n",
       "      <th>Financial Stress</th>\n",
       "      <th>Family History of Mental Illness</th>\n",
       "      <th>Depression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Male</td>\n",
       "      <td>33.0</td>\n",
       "      <td>Visakhapatnam</td>\n",
       "      <td>Student</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.97</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5-6 hours</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>B.Pharm</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>Female</td>\n",
       "      <td>24.0</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Student</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.90</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5-6 hours</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>BSc</td>\n",
       "      <td>No</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>Male</td>\n",
       "      <td>31.0</td>\n",
       "      <td>Srinagar</td>\n",
       "      <td>Student</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.03</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Less than 5 hours</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>BA</td>\n",
       "      <td>No</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>Female</td>\n",
       "      <td>28.0</td>\n",
       "      <td>Varanasi</td>\n",
       "      <td>Student</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.59</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7-8 hours</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>BCA</td>\n",
       "      <td>Yes</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>Female</td>\n",
       "      <td>25.0</td>\n",
       "      <td>Jaipur</td>\n",
       "      <td>Student</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.13</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5-6 hours</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>M.Tech</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  Gender   Age           City Profession  Academic Pressure  \\\n",
       "0   2    Male  33.0  Visakhapatnam    Student                5.0   \n",
       "1   8  Female  24.0      Bangalore    Student                2.0   \n",
       "2  26    Male  31.0       Srinagar    Student                3.0   \n",
       "3  30  Female  28.0       Varanasi    Student                3.0   \n",
       "4  32  Female  25.0         Jaipur    Student                4.0   \n",
       "\n",
       "   Work Pressure  CGPA  Study Satisfaction  Job Satisfaction  \\\n",
       "0            0.0  8.97                 2.0               0.0   \n",
       "1            0.0  5.90                 5.0               0.0   \n",
       "2            0.0  7.03                 5.0               0.0   \n",
       "3            0.0  5.59                 2.0               0.0   \n",
       "4            0.0  8.13                 3.0               0.0   \n",
       "\n",
       "      Sleep Duration Dietary Habits   Degree  \\\n",
       "0          5-6 hours        Healthy  B.Pharm   \n",
       "1          5-6 hours       Moderate      BSc   \n",
       "2  Less than 5 hours        Healthy       BA   \n",
       "3          7-8 hours       Moderate      BCA   \n",
       "4          5-6 hours       Moderate   M.Tech   \n",
       "\n",
       "  Have you ever had suicidal thoughts ?  Work/Study Hours  Financial Stress  \\\n",
       "0                                   Yes               3.0               1.0   \n",
       "1                                    No               3.0               2.0   \n",
       "2                                    No               9.0               1.0   \n",
       "3                                   Yes               4.0               5.0   \n",
       "4                                   Yes               1.0               1.0   \n",
       "\n",
       "  Family History of Mental Illness  Depression  \n",
       "0                               No           1  \n",
       "1                              Yes           0  \n",
       "2                              Yes           0  \n",
       "3                              Yes           1  \n",
       "4                               No           0  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stud_df = pd.read_csv('data/Student Depression Dataset.csv')\n",
    "stud_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка данных для классификации\n",
    "\n",
    "Сначала из датасета копируется таблица и разделяются признаки и целевая переменная: столбец `Depression` берётся как метка класса, а `id` удаляется как технический идентификатор. Далее выборка разбивается на обучающую и тестовую части с `stratify=y_clf`, чтобы сохранить исходное распределение классов. Отдельно задаются списки категориальных и числовых признаков, после чего все пропуски в данных заполняются наиболее частыми значениями с помощью `SimpleImputer`, чтобы логистическая регрессия и собственная модель могли работать с полной матрицей без `NaN`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>City</th>\n",
       "      <th>Profession</th>\n",
       "      <th>Academic Pressure</th>\n",
       "      <th>Work Pressure</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Study Satisfaction</th>\n",
       "      <th>Job Satisfaction</th>\n",
       "      <th>Sleep Duration</th>\n",
       "      <th>Dietary Habits</th>\n",
       "      <th>Degree</th>\n",
       "      <th>Have you ever had suicidal thoughts ?</th>\n",
       "      <th>Work/Study Hours</th>\n",
       "      <th>Financial Stress</th>\n",
       "      <th>Family History of Mental Illness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Male</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Jaipur</td>\n",
       "      <td>Student</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7-8 hours</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>Class 12</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Male</td>\n",
       "      <td>25.0</td>\n",
       "      <td>Vadodara</td>\n",
       "      <td>Student</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.37</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7-8 hours</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>B.Arch</td>\n",
       "      <td>No</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Male</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Ahmedabad</td>\n",
       "      <td>Student</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.24</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7-8 hours</td>\n",
       "      <td>Unhealthy</td>\n",
       "      <td>M.Ed</td>\n",
       "      <td>Yes</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Male</td>\n",
       "      <td>34.0</td>\n",
       "      <td>Bhopal</td>\n",
       "      <td>Student</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.37</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7-8 hours</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>B.Com</td>\n",
       "      <td>Yes</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male</td>\n",
       "      <td>25.0</td>\n",
       "      <td>Patna</td>\n",
       "      <td>Student</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.47</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5-6 hours</td>\n",
       "      <td>Unhealthy</td>\n",
       "      <td>B.Com</td>\n",
       "      <td>No</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Gender   Age       City Profession Academic Pressure Work Pressure  CGPA  \\\n",
       "0   Male  18.0     Jaipur    Student               4.0           0.0  6.02   \n",
       "1   Male  25.0   Vadodara    Student               3.0           0.0  6.37   \n",
       "2   Male  30.0  Ahmedabad    Student               3.0           0.0  9.24   \n",
       "3   Male  34.0     Bhopal    Student               3.0           0.0  7.37   \n",
       "4   Male  25.0      Patna    Student               3.0           0.0  7.47   \n",
       "\n",
       "  Study Satisfaction Job Satisfaction Sleep Duration Dietary Habits    Degree  \\\n",
       "0                1.0              0.0      7-8 hours       Moderate  Class 12   \n",
       "1                2.0              0.0      7-8 hours       Moderate    B.Arch   \n",
       "2                2.0              0.0      7-8 hours      Unhealthy      M.Ed   \n",
       "3                5.0              0.0      7-8 hours       Moderate     B.Com   \n",
       "4                4.0              0.0      5-6 hours      Unhealthy     B.Com   \n",
       "\n",
       "  Have you ever had suicidal thoughts ? Work/Study Hours Financial Stress  \\\n",
       "0                                   Yes              3.0              5.0   \n",
       "1                                    No              9.0              1.0   \n",
       "2                                   Yes              5.0              5.0   \n",
       "3                                   Yes             12.0              3.0   \n",
       "4                                    No             11.0              5.0   \n",
       "\n",
       "  Family History of Mental Illness  \n",
       "0                               No  \n",
       "1                              Yes  \n",
       "2                              Yes  \n",
       "3                               No  \n",
       "4                               No  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_df = stud_df.copy()\n",
    "\n",
    "X_clf = clf_df.drop(columns=[\"Depression\", \"id\"])\n",
    "y_clf = clf_df[\"Depression\"]\n",
    "\n",
    "X_clf_train, X_clf_test, y_clf_train, y_clf_test = train_test_split(\n",
    "    X_clf,\n",
    "    y_clf,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_clf,\n",
    ")\n",
    "\n",
    "clf_categorical_cols = [\n",
    "    \"Gender\",\n",
    "    \"City\",\n",
    "    \"Profession\",\n",
    "    \"Sleep Duration\",\n",
    "    \"Dietary Habits\",\n",
    "    \"Degree\",\n",
    "    \"Have you ever had suicidal thoughts ?\",\n",
    "    \"Family History of Mental Illness\",\n",
    "]\n",
    "\n",
    "clf_numeric_cols = [\n",
    "    \"Age\",\n",
    "    \"Academic Pressure\",\n",
    "    \"Work Pressure\",\n",
    "    \"CGPA\",\n",
    "    \"Study Satisfaction\",\n",
    "    \"Job Satisfaction\",\n",
    "    \"Work/Study Hours\",\n",
    "    \"Financial Stress\",\n",
    "]\n",
    "\n",
    "clf_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "X_clf_train_imp = pd.DataFrame(\n",
    "    clf_imputer.fit_transform(X_clf_train),\n",
    "    columns=X_clf_train.columns,\n",
    ")\n",
    "X_clf_test_imp = pd.DataFrame(\n",
    "    clf_imputer.transform(X_clf_test),\n",
    "    columns=X_clf_test.columns,\n",
    ")\n",
    "\n",
    "X_clf_train_imp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Построение бейзлайна "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала категориальные признаки переводятся в числовой формат с помощью OrdinalEncoder, чтобы бустинг мог работать с ними как с обычными числовыми столбцами. Затем на таком представлении обучается и оценивается базовый GradientBoostingClassifier на кросс-валидации и тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Классификация: Gradient Boosting (train CV, бейзлайн) ===\n",
      "Accuracy_mean:  0.835\n",
      "Precision_mean:0.834\n",
      "Recall_mean:   0.835\n",
      "F1_mean:       0.834\n",
      "\n",
      "Классификация — Gradient Boosting (test, бейзлайн)\n",
      "--------------------------------------------------\n",
      "1. Accuracy:  82.85%\n",
      "2. Precision: 82.79%\n",
      "3. Recall:    82.85%\n",
      "4. F1-score:  82.80%\n"
     ]
    }
   ],
   "source": [
    "clf_ordinal_encoder = OrdinalEncoder(\n",
    "    handle_unknown=\"use_encoded_value\",\n",
    "    unknown_value=99,\n",
    ")\n",
    "\n",
    "X_clf_train_base = X_clf_train_imp.copy()\n",
    "X_clf_test_base = X_clf_test_imp.copy()\n",
    "\n",
    "X_clf_train_base[clf_categorical_cols] = clf_ordinal_encoder.fit_transform(\n",
    "    X_clf_train_base[clf_categorical_cols]\n",
    ")\n",
    "X_clf_test_base[clf_categorical_cols] = clf_ordinal_encoder.transform(\n",
    "    X_clf_test_base[clf_categorical_cols]\n",
    ")\n",
    "\n",
    "gb_clf_base_mean, gb_clf_base_std = classification_cross_validate(\n",
    "    GradientBoostingClassifier,\n",
    "    X_clf_train_base.to_numpy(),\n",
    "    y_clf_train.to_numpy(),\n",
    "    n_folds=5,\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    ")\n",
    "\n",
    "print(\"=== Классификация: Gradient Boosting (train CV, бейзлайн) ===\")\n",
    "print(f\"Accuracy_mean:  {gb_clf_base_mean['Accuracy']:.3f}\")\n",
    "print(f\"Precision_mean:{gb_clf_base_mean['Precision']:.3f}\")\n",
    "print(f\"Recall_mean:   {gb_clf_base_mean['Recall']:.3f}\")\n",
    "print(f\"F1_mean:       {gb_clf_base_mean['F1-score']:.3f}\")\n",
    "\n",
    "gb_clf_baseline = GradientBoostingClassifier(\n",
    "    random_state=42,\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    ")\n",
    "gb_clf_baseline.fit(X_clf_train_base, y_clf_train)\n",
    "\n",
    "y_clf_pred_gb_base = gb_clf_baseline.predict(X_clf_test_base)\n",
    "\n",
    "acc_gb_base = accuracy_score(y_clf_test, y_clf_pred_gb_base)\n",
    "prec_gb_base = precision_score(\n",
    "    y_clf_test,\n",
    "    y_clf_pred_gb_base,\n",
    "    average=\"weighted\",\n",
    "    zero_division=0,\n",
    ")\n",
    "rec_gb_base = recall_score(\n",
    "    y_clf_test,\n",
    "    y_clf_pred_gb_base,\n",
    "    average=\"weighted\",\n",
    "    zero_division=0,\n",
    ")\n",
    "f1_gb_base = f1_score(\n",
    "    y_clf_test,\n",
    "    y_clf_pred_gb_base,\n",
    "    average=\"weighted\",\n",
    "    zero_division=0,\n",
    ")\n",
    "\n",
    "print(\"\\nКлассификация — Gradient Boosting (test, бейзлайн)\")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(f\"1. Accuracy:  {acc_gb_base:.2%}\")\n",
    "print(f\"2. Precision: {prec_gb_base:.2%}\")\n",
    "print(f\"3. Recall:    {rec_gb_base:.2%}\")\n",
    "print(f\"4. F1-score:  {f1_gb_base:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Формулировка гипотез"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Гипотезы по улучшению признакового пространства для классификации: категориальные признаки переводятся в one-hot представление, а числовые столбцы дополнительно масштабируются. В результате получаются обновлённые обучающая и тестовая матрицы признаков, на которых далее можно обучать более аккуратно настроенный градиентный бустинг с изменённой глубиной и числом деревье"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:246: UserWarning: Found unknown categories in columns [1, 2] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "gb_clf_onehot = OneHotEncoder(\n",
    "    sparse_output=False,\n",
    "    drop=\"first\",\n",
    "    handle_unknown=\"ignore\",\n",
    ")\n",
    "\n",
    "gb_clf_cat_train_ohe = gb_clf_onehot.fit_transform(\n",
    "    X_clf_train_imp[clf_categorical_cols]\n",
    ")\n",
    "gb_clf_cat_test_ohe = gb_clf_onehot.transform(\n",
    "    X_clf_test_imp[clf_categorical_cols]\n",
    ")\n",
    "\n",
    "gb_clf_ohe_cols = gb_clf_onehot.get_feature_names_out(clf_categorical_cols)\n",
    "\n",
    "X_clf_train_gb = pd.concat(\n",
    "    [\n",
    "        X_clf_train_imp[clf_numeric_cols].reset_index(drop=True),\n",
    "        pd.DataFrame(gb_clf_cat_train_ohe, columns=gb_clf_ohe_cols),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "X_clf_test_gb = pd.concat(\n",
    "    [\n",
    "        X_clf_test_imp[clf_numeric_cols].reset_index(drop=True),\n",
    "        pd.DataFrame(gb_clf_cat_test_ohe, columns=gb_clf_ohe_cols),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "gb_clf_scaler = StandardScaler()\n",
    "X_clf_train_gb[clf_numeric_cols] = gb_clf_scaler.fit_transform(\n",
    "    X_clf_train_gb[clf_numeric_cols]\n",
    ")\n",
    "X_clf_test_gb[clf_numeric_cols] = gb_clf_scaler.transform(\n",
    "    X_clf_test_gb[clf_numeric_cols]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Строится улучшенная версия градиентного бустинга на обновлённых признаках с one-hot кодированием и масштабированием числовых столбцов. Сначала качество оценивается по кросс-валидации, затем модель обучается на всей обучающей выборке и проверяется на тесте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Классификация: Gradient Boosting (train CV, улучшенный бейзлайн) ===\n",
      "Accuracy_mean:  0.848\n",
      "Precision_mean:0.848\n",
      "Recall_mean:   0.848\n",
      "F1_mean:       0.848\n",
      "\n",
      "Классификация — Gradient Boosting (test, улучшенный бейзлайн)\n",
      "--------------------------------------------------------------\n",
      "1. Accuracy:  84.29%\n",
      "2. Precision: 84.23%\n",
      "3. Recall:    84.29%\n",
      "4. F1-score:  84.23%\n"
     ]
    }
   ],
   "source": [
    "gb_clf_imp_mean, gb_clf_imp_std = classification_cross_validate(\n",
    "    GradientBoostingClassifier,\n",
    "    X_clf_train_gb.to_numpy(),\n",
    "    y_clf_train.to_numpy(),\n",
    "    n_folds=5,\n",
    "    random_state=42,\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    ")\n",
    "\n",
    "print(\"=== Классификация: Gradient Boosting (train CV, улучшенный бейзлайн) ===\")\n",
    "print(f\"Accuracy_mean:  {gb_clf_imp_mean['Accuracy']:.3f}\")\n",
    "print(f\"Precision_mean:{gb_clf_imp_mean['Precision']:.3f}\")\n",
    "print(f\"Recall_mean:   {gb_clf_imp_mean['Recall']:.3f}\")\n",
    "print(f\"F1_mean:       {gb_clf_imp_mean['F1-score']:.3f}\")\n",
    "\n",
    "gb_clf_improved = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    random_state=42,\n",
    ")\n",
    "gb_clf_improved.fit(X_clf_train_gb, y_clf_train)\n",
    "\n",
    "y_clf_pred_gb_imp = gb_clf_improved.predict(X_clf_test_gb)\n",
    "\n",
    "acc_gb_imp = accuracy_score(y_clf_test, y_clf_pred_gb_imp)\n",
    "prec_gb_imp = precision_score(\n",
    "    y_clf_test,\n",
    "    y_clf_pred_gb_imp,\n",
    "    average=\"weighted\",\n",
    "    zero_division=0,\n",
    ")\n",
    "rec_gb_imp = recall_score(\n",
    "    y_clf_test,\n",
    "    y_clf_pred_gb_imp,\n",
    "    average=\"weighted\",\n",
    "    zero_division=0,\n",
    ")\n",
    "f1_gb_imp = f1_score(\n",
    "    y_clf_test,\n",
    "    y_clf_pred_gb_imp,\n",
    "    average=\"weighted\",\n",
    "    zero_division=0,\n",
    ")\n",
    "\n",
    "print(\"\\nКлассификация — Gradient Boosting (test, улучшенный бейзлайн)\")\n",
    "print(\"--------------------------------------------------------------\")\n",
    "print(f\"1. Accuracy:  {acc_gb_imp:.2%}\")\n",
    "print(f\"2. Precision: {prec_gb_imp:.2%}\")\n",
    "print(f\"3. Recall:    {rec_gb_imp:.2%}\")\n",
    "print(f\"4. F1-score:  {f1_gb_imp:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Реализация своего класса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем бинарный градиентный бустинг в логистической постановке: модель стартует с константного логита, а затем деревья последовательно аппроксимируют остатки между истинными метками и текущими вероятностями. На этапе предсказания модель накапливает поправки в пространстве логитов и переводит их в вероятности через сигмоиду, после чего принимает решение по фиксированному порогу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGradientBoostingClassifier:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_estimators: int = 100,\n",
    "        learning_rate: float = 0.1,\n",
    "        max_depth: int = 3,\n",
    "    ):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "        self.trees: list[DecisionTreeRegressor] = []\n",
    "        self.initial_value: float | None = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        z = np.asarray(z, dtype=float)\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    def _log_odds(self, y):\n",
    "        y = np.asarray(y, dtype=float)\n",
    "        p = np.clip(np.mean(y), 1e-15, 1.0 - 1e-15)\n",
    "        return float(np.log(p / (1.0 - p)))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        y = np.asarray(y, dtype=float)\n",
    "\n",
    "        self.initial_value = self._log_odds(y)\n",
    "        residual = y - self._sigmoid(self.initial_value)\n",
    "        self.trees = []\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residual)\n",
    "            pred = tree.predict(X)\n",
    "            residual = residual - self.learning_rate * pred\n",
    "            self.trees.append(tree)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "\n",
    "        if self.initial_value is None:\n",
    "            raise RuntimeError(\"Сначала нужно вызвать fit().\")\n",
    "\n",
    "        log_odds = np.full(X.shape[0], self.initial_value, dtype=float)\n",
    "        for tree in self.trees:\n",
    "            log_odds = log_odds + self.learning_rate * tree.predict(X)\n",
    "\n",
    "        proba_pos = self._sigmoid(log_odds)\n",
    "        return np.vstack([1.0 - proba_pos, proba_pos]).T\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return (proba[:, 1] > 0.6).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оцениваем базовая версия собственной реализации градиентного бустинга на признаках с порядковым кодированием категорий и заполненными пропусками. Сначала качество проверяется по кросс-валидации, затем та же конфигурация тестируется на отложенной выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Классификация: собственный Gradient Boosting (train CV, без улучшений) ===\n",
      "Accuracy_mean:  0.821\n",
      "Precision_mean:0.827\n",
      "Recall_mean:   0.821\n",
      "F1_mean:       0.822\n",
      "\n",
      "Классификация — собственный Gradient Boosting (test, без улучшений)\n",
      "-------------------------------------------------------------------\n",
      "1. Accuracy:  82.15%\n",
      "2. Precision: 82.81%\n",
      "3. Recall:    82.15%\n",
      "4. F1-score:  82.26%\n"
     ]
    }
   ],
   "source": [
    "gb_my_clf_base_mean, gb_my_clf_base_std = classification_cross_validate(\n",
    "    MyGradientBoostingClassifier,\n",
    "    X_clf_train_base.to_numpy(),\n",
    "    y_clf_train.to_numpy(),\n",
    "    n_folds=5,\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    ")\n",
    "\n",
    "print(\"=== Классификация: собственный Gradient Boosting (train CV, без улучшений) ===\")\n",
    "print(f\"Accuracy_mean:  {gb_my_clf_base_mean['Accuracy']:.3f}\")\n",
    "print(f\"Precision_mean:{gb_my_clf_base_mean['Precision']:.3f}\")\n",
    "print(f\"Recall_mean:   {gb_my_clf_base_mean['Recall']:.3f}\")\n",
    "print(f\"F1_mean:       {gb_my_clf_base_mean['F1-score']:.3f}\")\n",
    "\n",
    "my_gb_clf_base = MyGradientBoostingClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    ")\n",
    "my_gb_clf_base.fit(X_clf_train_base.to_numpy(), y_clf_train.to_numpy())\n",
    "\n",
    "y_clf_pred_my_gb_base = my_gb_clf_base.predict(X_clf_test_base.to_numpy())\n",
    "\n",
    "acc_my_gb_base = accuracy_score(y_clf_test, y_clf_pred_my_gb_base)\n",
    "prec_my_gb_base = precision_score(\n",
    "    y_clf_test,\n",
    "    y_clf_pred_my_gb_base,\n",
    "    average=\"weighted\",\n",
    "    zero_division=0,\n",
    ")\n",
    "rec_my_gb_base = recall_score(\n",
    "    y_clf_test,\n",
    "    y_clf_pred_my_gb_base,\n",
    "    average=\"weighted\",\n",
    "    zero_division=0,\n",
    ")\n",
    "f1_my_gb_base = f1_score(\n",
    "    y_clf_test,\n",
    "    y_clf_pred_my_gb_base,\n",
    "    average=\"weighted\",\n",
    "    zero_division=0,\n",
    ")\n",
    "\n",
    "print(\"\\nКлассификация — собственный Gradient Boosting (test, без улучшений)\")\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(f\"1. Accuracy:  {acc_my_gb_base:.2%}\")\n",
    "print(f\"2. Precision: {prec_my_gb_base:.2%}\")\n",
    "print(f\"3. Recall:    {rec_my_gb_base:.2%}\")\n",
    "print(f\"4. F1-score:  {f1_my_gb_base:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собственный градиентный бустинг обучается на улучшенных признаках с one-hot кодированием и масштабированием числовых столбцов. Сначала качество оценивается по кросс-валидации, затем та же модель проверяется на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Классификация: собственный Gradient Boosting (train CV, с улучшениями) ===\n",
      "Accuracy_mean:  0.834\n",
      "Precision_mean:0.842\n",
      "Recall_mean:   0.834\n",
      "F1_mean:       0.835\n",
      "\n",
      "Классификация — собственный Gradient Boosting (test, с улучшениями)\n",
      "-------------------------------------------------------------------\n",
      "1. Accuracy:  82.89%\n",
      "2. Precision: 83.76%\n",
      "3. Recall:    82.89%\n",
      "4. F1-score:  83.01%\n"
     ]
    }
   ],
   "source": [
    "gb_my_clf_imp_mean, gb_my_clf_imp_std = classification_cross_validate(\n",
    "    MyGradientBoostingClassifier,\n",
    "    X_clf_train_gb.to_numpy(),\n",
    "    y_clf_train.to_numpy(),\n",
    "    n_folds=5,\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    ")\n",
    "\n",
    "print(\"=== Классификация: собственный Gradient Boosting (train CV, с улучшениями) ===\")\n",
    "print(f\"Accuracy_mean:  {gb_my_clf_imp_mean['Accuracy']:.3f}\")\n",
    "print(f\"Precision_mean:{gb_my_clf_imp_mean['Precision']:.3f}\")\n",
    "print(f\"Recall_mean:   {gb_my_clf_imp_mean['Recall']:.3f}\")\n",
    "print(f\"F1_mean:       {gb_my_clf_imp_mean['F1-score']:.3f}\")\n",
    "\n",
    "my_gb_clf_improved = MyGradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    ")\n",
    "my_gb_clf_improved.fit(X_clf_train_gb.to_numpy(), y_clf_train.to_numpy())\n",
    "\n",
    "y_clf_pred_my_gb_imp = my_gb_clf_improved.predict(X_clf_test_gb.to_numpy())\n",
    "\n",
    "acc_my_gb_imp = accuracy_score(y_clf_test, y_clf_pred_my_gb_imp)\n",
    "prec_my_gb_imp = precision_score(\n",
    "    y_clf_test,\n",
    "    y_clf_pred_my_gb_imp,\n",
    "    average=\"weighted\",\n",
    "    zero_division=0,\n",
    ")\n",
    "rec_my_gb_imp = recall_score(\n",
    "    y_clf_test,\n",
    "    y_clf_pred_my_gb_imp,\n",
    "    average=\"weighted\",\n",
    "    zero_division=0,\n",
    ")\n",
    "f1_my_gb_imp = f1_score(\n",
    "    y_clf_test,\n",
    "    y_clf_pred_my_gb_imp,\n",
    "    average=\"weighted\",\n",
    "    zero_division=0,\n",
    ")\n",
    "\n",
    "print(\"\\nКлассификация — собственный Gradient Boosting (test, с улучшениями)\")\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(f\"1. Accuracy:  {acc_my_gb_imp:.2%}\")\n",
    "print(f\"2. Precision: {prec_my_gb_imp:.2%}\")\n",
    "print(f\"3. Recall:    {rec_my_gb_imp:.2%}\")\n",
    "print(f\"4. F1-score:  {f1_my_gb_imp:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заключение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В задаче регрессии видно, что и для `GradientBoostingRegressor` из sklearn, и для собственной реализации улучшенный препроцессинг (one-hot + масштабирование) даёт прирост качества на тестовой выборке: ошибки снижаются, а R^2 немного растёт. Особенно заметно, что самописный бустинг после улучшений по R^2 даже превосходит базовый sklearn-вариант. Для классификации улучшенный вариант признаков также даёт прирост метрик у библиотечной модели, тогда как собственная реализация реагирует мягче и показывает близкое, но чуть менее стабильное улучшение. В целом, и для регрессии, и для классификации видно, что гипотезы про one-hot кодирование и масштабирование в сочетании с перенастройкой глубины/числа деревьев в основном оправдываются и для sklearn, и для самописных моделей\n",
    "\n",
    "### Сводная таблица по регрессии (test)\n",
    "\n",
    "| Модель                                      |      MSE |      MAE | R^2 |\n",
    "| :------------------------------------------ | -------: | -------: | :---: |\n",
    "| Sklearn (до улучшения)                      | 2.48e+08 |  9997.82 | 0.897 |\n",
    "| Sklearn (после улучшения)                   | 2.20e+08 |  9084.95 | 0.908 |\n",
    "| Собственная имплементация (до улучшения)    | 2.33e+08 | 10172.64 | 0.903 |\n",
    "| Собственная имплементация (после улучшения) | 2.14e+08 |  9058.04 | 0.911 |\n",
    "\n",
    "### Сводная таблица по классификации (test)\n",
    "\n",
    "| Модель                                      | Accuracy | Precision | Recall | F1-score |\n",
    "| :------------------------------------------ | -------: | --------: | -----: | -------: |\n",
    "| Sklearn (до улучшения)                      |   82.85% |    82.79% | 82.85% |   82.80% |\n",
    "| Sklearn (после улучшения)                   |   84.29% |    84.23% | 84.29% |   84.23% |\n",
    "| Собственная имплементация (до улучшения)    |   82.40% |    83.08% | 82.40% |   82.52% |\n",
    "| Собственная имплементация (после улучшения) |   82.89% |    83.76% | 82.89% |   83.01% |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
